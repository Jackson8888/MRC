### Attention
Query, Key, Value  
每个词都有一个原始的Value  
Key, 上下文词  
Value, 上下文词的value  
==>> 增强语义向量  


### Transformer
[Attention is All you Need]
attention=>>self-attention==>> multi-head self-attention==>> transformer(encoder + decoder)  

- transformer encoder
    + Multi-head Self-Attention
    + Layer Normalization
    + linear transfor

### QANet 
[QANet: Combining local convolutional with glogal self-attention]()
